{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clustering and Regression -  Daily Electrical Load Profiles and Prediction.ipynb","provenance":[],"collapsed_sections":["cZVDndNrGEHH"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qHKWVStEq9D8","colab_type":"text"},"source":["# Introduction to Machine Learning for the Built Environment - Unsupervised Learning using Clustering and Supervised Prediction using Regression\n","\n","- Created by Clayton Miller - clayton@nus.edu.sg - miller.clayton@gmail.com\n","\n","This notebook is an introduction to the machine learning concepts of clustering and preduction using regression. We will use the Building Data Genome Project data set to analyze electrical meter data from non-residential buildings."]},{"cell_type":"markdown","metadata":{"id":"F0eW5Ycmre_c","colab_type":"text"},"source":["## The Scikit Learn Machine Learning Library\n","\n","In this series of videos, we will learn a new library called Scikit-Learn that includes various Machine Learning Models:\n","\n","### https://scikit-learn.org/stable/\n","\n","![alt text](https://raw.githubusercontent.com/buds-lab/the-building-data-genome-project/master/docs/edx-graphics/EDX-ML-ScikitLearn-2.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y6rF--U29kLN","colab_type":"text"},"source":["## Scikit-Learn Cheat Sheet\n","\n","A handy flow chart is available open source from the scikit-learn community from: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n","\n","![alt text](https://raw.githubusercontent.com/buds-lab/the-building-data-genome-project/master/docs/edx-graphics/EDX-ML-ScikitLearn-1.png)\n"]},{"cell_type":"markdown","metadata":{"id":"LXteyZoDuApU","colab_type":"text"},"source":["## Using the Building Data Genome Project Data Set for Clustering and Regression Prediction\n","\n","Let's use the lectrical meter data to create clusters of typical load profiles for analysis. First we can load our conventional packages"]},{"cell_type":"code","metadata":{"id":"l2OZzd4c6yHk","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6uiTF8u8TDM","colab_type":"text"},"source":["Next let's load all the packages we will need for the clustering and regression analysis"]},{"cell_type":"code","metadata":{"id":"50EWscxn8SaJ","colab_type":"code","colab":{}},"source":["import sklearn\n","from sklearn import metrics\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","from scipy.cluster.vq import kmeans, vq, whiten\n","from scipy.spatial.distance import cdist\n","import numpy as np\n","from datetime import datetime\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QpXQPGENDSA9","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHi3V9_KsBUe","colab_type":"code","colab":{}},"source":["os.chdir(\"/content/gdrive/My Drive/EDX Data Science for Construction, Architecture and Engineering/3 - Construction - Pandas Fundamentals/meter_data/\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"05mulZjStn3n","colab_type":"text"},"source":["# Using Unsupervised Learning to Cluster Daily Load Profiles\n","\n","The first thing we will use the library for is to analyze the daily load profiles from an electrical meter\n","\n","Let's load an example meter data file and do a clustering analysis of the data. We will be following the tutorial found on the [Data-Driven Building](https://cargocollective.com/buildingdata/DayFilter-Unsupervised-Pattern-Filtering) blog.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jcgXmLYs0irH","colab_type":"text"},"source":["## Load meter data from a single building"]},{"cell_type":"code","metadata":{"id":"cTUGVQKC7XiC","colab_type":"code","colab":{}},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQOg0XfMth-M","colab_type":"code","colab":{}},"source":["df = pd.read_csv('Office_Amelie.csv', index_col = \"timestamp\", parse_dates=True) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owF9iqqOy0HT","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gKNXkwP0ISg","colab_type":"code","colab":{}},"source":["df.plot(alpha=0.5, figsize=(15, 5))\n","plt.title(\"Electricity Consumption\")\n","plt.xlabel(\"Time Range\")\n","plt.ylabel(\"kWh Electricity Consumption Visualization\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zw2hegWN8nUM","colab_type":"text"},"source":["Let's zoom in on smaller time range to see more detailed patterns"]},{"cell_type":"code","metadata":{"id":"Kb7eaus30oFo","colab_type":"code","colab":{}},"source":["df.truncate(before='01-02-2015', after='14-02-2015').plot(figsize=(15,5))\n","plt.title(\"Electricity Consumption\")\n","plt.xlabel(\"Time Range\")\n","plt.ylabel(\"kWh Electricity Consumption Visualization\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m4Wntj3p9CO_","colab_type":"text"},"source":["## Conventional Daily Profile Analysis - Weekday vs. Weekend\n","\n","It appears that there is some standard weekday vs. weekend behaviour and a few basic types of daily patterns.\n","\n","Let's first do it the conventional way by looking at all the daily profiles. We'll pivot to get a DataFrame that can be plotted the way we needed."]},{"cell_type":"code","metadata":{"id":"n8Vt0zSx84t3","colab_type":"code","colab":{}},"source":["df['Date'] = df.index.map(lambda t: t.date())\n","df['Time'] = df.index.map(lambda t: t.time())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"imqNY9wf9Jmn","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtGx2kbZ9Kj8","colab_type":"code","colab":{}},"source":["df_pivot = pd.pivot_table(df, values='Office_Amelie', index='Date', columns='Time')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2qmQx0k9Nkn","colab_type":"code","colab":{}},"source":["df_pivot.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgNt00e79REh","colab_type":"code","colab":{}},"source":["df_pivot.T.plot(legend=False, figsize=(15,5), color='k', alpha=0.1, xticks=np.arange(0, 86400, 10800))\n","plt.title(\"Electrical Meter Data - Daily Profiles\")\n","plt.xlabel(\"Daily Time Frame\")\n","plt.ylabel(\"kWh Electricity\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WMfbk0fS9hxf","colab_type":"text"},"source":["Looks like we have quite a few pretty common patterns and a few outlier patterns where we have some consumption in the early morning and late night hours.\n","\n","How can we divide this dataset up according to conventional wisdom -- the first obvious choice is to divide between weekdays vs. the weekends.\n","\n","Let's look at weekdays first:"]},{"cell_type":"code","metadata":{"id":"74O072_V9aNo","colab_type":"code","colab":{}},"source":["df['Weekday'] = df.index.map(lambda t: t.date().weekday())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiSt202k-DFC","colab_type":"code","colab":{}},"source":["df.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5hDPVs2-El6","colab_type":"code","colab":{}},"source":["df_pivot_weekday = pd.pivot_table(df[(df.Weekday < 5)], values='Office_Amelie', index='Date', columns='Time')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQ-8gy2g-IDy","colab_type":"code","colab":{}},"source":["df_pivot_weekday.T.plot(legend=False, figsize=(15,5), color='k', alpha=0.1, xticks=np.arange(0, 86400, 10800))\n","plt.title(\"Electrical Meter Data - Weekday Daily Profiles\")\n","plt.xlabel(\"Daily Time Frame\")\n","plt.ylabel(\"kWh Electricity\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqMORmUA-qf9","colab_type":"text"},"source":["It can be noticed that there is still quite a bit of anomolous-looking daily profiles that are not characterized only by the day of the week -- this can be due to holidays, weird schedules, or actually deviant behaviour.\n","\n","\n","\n","## Manual indentification of clusters\n","\n","There also seems to be varying levels of consumption throughout the course of a year. This is likely because of weather effects or schedule changes. \n","\n","These could be considered \"clusters\" of behavior due to the course of \n","\n","Let's try weekend:"]},{"cell_type":"code","metadata":{"id":"hTYYi10P-PCg","colab_type":"code","colab":{}},"source":["df_pivot_weekend = pd.pivot_table(df[(df.Weekday > 5)], values='Office_Amelie', index='Date', columns='Time')\n","df_pivot_weekend.T.plot(legend=False, figsize=(15,5), color='k', alpha=0.1, xticks=np.arange(0, 86400, 10800))\n","plt.title(\"Electrical Meter Data - Weekday Daily Profiles\")\n","plt.xlabel(\"Daily Time Frame\")\n","plt.ylabel(\"kWh Electricity\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IpvbCmXKEY7W","colab_type":"text"},"source":["Weekends have a lower standard level of consumption with only bits of consumption during daytime hours\n","\n","## k-Means Clustering of Daily Load Profiles\n","\n","Let's reload the dataframe to start over so we can do the k-means process"]},{"cell_type":"code","metadata":{"id":"2iq8sE_1Hxvb","colab_type":"code","colab":{}},"source":["df = pd.read_csv('Office_Amelie.csv', index_col = \"timestamp\", parse_dates=True) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QcGBhqKHmQE","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hzua9pD-cQR","colab_type":"code","colab":{}},"source":["df_norm = (df - df.mean()) / (df.max() - df.min()) \n","\n","df['Time'] = df.index.map(lambda t: t.time())\n","df['Date'] = df.index.map(lambda t: t.date())\n","df_norm['Time'] = df_norm.index.map(lambda t: t.time())\n","df_norm['Date'] = df_norm.index.map(lambda t: t.date())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Vm7Ov0sJUR5","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEgUiK4xHcAV","colab_type":"code","colab":{}},"source":["dailyblocks = pd.pivot_table(df, values='Office_Amelie', index='Date', columns='Time', aggfunc='mean')\n","dailyblocks_norm = pd.pivot_table(df_norm, values='Office_Amelie', index='Date', columns='Time', aggfunc='mean')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UTC0mqZnH56g","colab_type":"code","colab":{}},"source":["dailyblocks_norm.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qRtBrFSRJ7yf","colab_type":"text"},"source":["## The Clustering Model\n","\n","There is no need to train an unsupervised model, but we do need to indicate how many clusters we would like the model to extract -- in this case we will use 4"]},{"cell_type":"code","metadata":{"id":"IASvS80zIEZc","colab_type":"code","colab":{}},"source":["dailyblocksmatrix_norm = np.matrix(dailyblocks_norm.dropna())\n","centers, _ = kmeans(dailyblocksmatrix_norm, 4, iter=10000)\n","cluster, _ = vq(dailyblocksmatrix_norm, centers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1agS59sIHSv","colab_type":"code","colab":{}},"source":["clusterdf = pd.DataFrame(cluster, columns=['ClusterNo'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SujvGyUgIJf9","colab_type":"code","colab":{}},"source":["dailyclusters = pd.concat([dailyblocks.dropna().reset_index(), clusterdf], axis=1) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwkgjRWtILYs","colab_type":"code","colab":{}},"source":["dailyclusters.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8swR9UPtIRvq","colab_type":"text"},"source":["Notice the last column is the cluster number assigned by the k-means process. We'll first reorder the clustering numbers so that the greatest consuming clusters have the highest numbers:"]},{"cell_type":"code","metadata":{"id":"SvE22oXNIM77","colab_type":"code","colab":{}},"source":["x = dailyclusters.groupby('ClusterNo').mean().sum(axis=1).sort_values()\n","x = pd.DataFrame(x.reset_index())\n","x['ClusterNo2'] = x.index\n","x = x.set_index('ClusterNo')\n","x = x.drop([0], axis=1)\n","dailyclusters = dailyclusters.merge(x, how='outer', left_on='ClusterNo', right_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iV8vi-FqIUkE","colab_type":"code","colab":{}},"source":["dailyclusters = dailyclusters.drop(['ClusterNo'],axis=1)\n","dailyclusters = dailyclusters.set_index(['ClusterNo2','Date']).T.sort_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYkiXbkxIWZK","colab_type":"code","colab":{}},"source":["dailyclusters.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qRTnk3wIZ1e","colab_type":"text"},"source":["Now we have a dataframe with each of the clusters hiearchically divided -- let's visualize what the clusters. First, let's look at all the profiles at once divided according to cluster:"]},{"cell_type":"code","metadata":{"id":"Sm91fcfaIXnX","colab_type":"code","colab":{}},"source":["clusterlist = list(dailyclusters.columns.get_level_values(0).unique())\n","matplotlib.rcParams['figure.figsize'] = 20, 7\n","\n","styles2 = ['LightSkyBlue', 'b','LightGreen', 'g','LightCoral','r','SandyBrown','Orange','Plum','Purple','Gold','b']\n","fig, ax = plt.subplots()\n","for col, style in zip(clusterlist, styles2):\n","    dailyclusters[col].plot(ax=ax, legend=False, style=style, alpha=0.1, xticks=np.arange(0, 86400, 10800))\n","\n","ax.set_ylabel('Total Daily Profile')\n","ax.set_xlabel('Time of Day');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LW8LK5zpIwxz","colab_type":"text"},"source":["## Aggregate visualizations of the clusters\n","\n","Now, let's aggregate and visualize the clusters as they exist across the time range:\n","\n"]},{"cell_type":"code","metadata":{"id":"-NwhRp65I4NO","colab_type":"code","colab":{}},"source":["def timestampcombine(date,time):\n","    pydatetime = datetime.combine(date, time)\n","    return pydatetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIRSOF2KIc5F","colab_type":"code","colab":{}},"source":["def ClusterUnstacker(df):\n","    df = df.unstack().reset_index()\n","    df['timestampstring'] = pd.to_datetime(df.Date.astype(\"str\") + \" \" + df.level_2.astype(\"str\"))\n","    #pd.to_datetime(df.Date  df.level_2) #map(timestampcombine, )\n","    df = df.dropna()\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"79dPnmBQI6pS","colab_type":"code","colab":{}},"source":["dailyclusters.unstack().reset_index().head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXOhYMLbI8Xg","colab_type":"code","colab":{}},"source":["dfclusterunstacked = ClusterUnstacker(dailyclusters)\n","dfclusterunstackedpivoted = pd.pivot_table(dfclusterunstacked, values=0, index='timestampstring', columns='ClusterNo2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ormS659P6n5r","colab_type":"code","colab":{}},"source":["clusteravgplot = dfclusterunstackedpivoted.resample('D').sum().replace(0, np.nan).plot(style=\"^\",markersize=15)\n","clusteravgplot.set_ylabel('Daily Totals kWh')\n","clusteravgplot.set_xlabel('Date');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"McGoO3FTKBVv","colab_type":"code","colab":{}},"source":["dfclusterunstackedpivoted['Time'] = dfclusterunstackedpivoted.index.map(lambda t: t.time())\n","dailyprofile = dfclusterunstackedpivoted.groupby('Time').mean().plot(figsize=(20,7),linewidth=3, xticks=np.arange(0, 86400, 10800))\n","dailyprofile.set_ylabel('Average Daily Profile kWh')\n","dailyprofile.set_xlabel('Time of Day')\n","dailyprofile.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Cluster')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XK9-yrkTNBnN","colab_type":"code","colab":{}},"source":["def DayvsClusterMaker(df):\n","    df.index = df.timestampstring\n","    df['Weekday'] = df.index.map(lambda t: t.date().weekday())\n","    df['Date'] = df.index.map(lambda t: t.date())\n","    df['Time'] = df.index.map(lambda t: t.time())\n","    DayVsCluster = df.resample('D').mean().reset_index(drop=True)\n","    DayVsCluster = pd.pivot_table(DayVsCluster, values=0, index='ClusterNo2', columns='Weekday', aggfunc='count')\n","    DayVsCluster.columns = ['Mon','Tue','Wed','Thur','Fri','Sat','Sun']\n","    return DayVsCluster.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YmnCttGONkW6","colab_type":"code","colab":{}},"source":["DayVsCluster = DayvsClusterMaker(dfclusterunstacked)\n","DayVsClusterplot1 = DayVsCluster.plot(figsize=(20,7),kind='bar',stacked=True)\n","DayVsClusterplot1.set_ylabel('Number of Days in Each Cluster')\n","DayVsClusterplot1.set_xlabel('Day of the Week')\n","DayVsClusterplot1.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Cluster')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAjV0vhtNnG4","colab_type":"code","colab":{}},"source":["DayVsClusterplot2 = DayVsCluster.T.plot(figsize=(20,7),kind='bar',stacked=True, color=['b','g','r','c','m','y','k']) #, color=colors2\n","DayVsClusterplot2.set_ylabel('Number of Days in Each Cluster')\n","DayVsClusterplot2.set_xlabel('Cluster Number')\n","DayVsClusterplot2.legend(loc='center left', bbox_to_anchor=(1, 0.5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yAfj2AZsOvKX","colab_type":"text"},"source":["# Electricity Prediction using Regression for Measurement and Verification\n","\n","Prediction is a common machine learning (ML) technique used on building energy consumption data. This process is valuable for anomaly detection, load profile-based building control and measurement and verification procedures. \n","\n","The graphic below comes from the IPMVP to show how prediction can be used for M&V to calculate how much energy **would have** been consumed if an energy savings intervention had not been implemented. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"cZVDndNrGEHH","colab_type":"text"},"source":["## Prediction for Measurement and Verification\n","\n","![alt text](https://raw.githubusercontent.com/buds-lab/the-building-data-genome-project/master/docs/edx-graphics/EDX-ML-ScikitLearn-3.png)\n","\n","There is an open publication that gives more information on how prediction in this realm can be approached: https://www.mdpi.com/2504-4990/1/3/56\n","\n","There is an entire Kaggle Machine Learning competition also focused on this application: https://www.kaggle.com/c/ashrae-energy-prediction\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eBMZrSlhRwQY","colab_type":"text"},"source":["## Load electricity data and weather data\n","\n","First we can load the data from the BDG in the same as our previous weather analysis influence notebook from the Construction Phase videos"]},{"cell_type":"code","metadata":{"id":"gKMx2yMqNsLm","colab_type":"code","colab":{}},"source":["df_prediction_data = pd.read_csv(\"UnivClass_Ciara.csv\", parse_dates=True, index_col='timestamp')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cM_QHlIbRX44","colab_type":"code","colab":{}},"source":["df_prediction_data.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7b5goOjQvoJ","colab_type":"code","colab":{}},"source":["os.chdir(\"/content/gdrive/My Drive/EDX Data Science for Construction, Architecture and Engineering/3 - Construction - Pandas Fundamentals/weather_data/\")\n","weather_data = pd.read_csv(\"weather2.csv\", index_col='timestamp', parse_dates=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7OzdoOmYQ2tp","colab_type":"code","colab":{}},"source":["weather_hourly = weather_data.resample(\"H\").mean()\n","weather_hourly_nooutlier = weather_hourly[weather_hourly > -40]\n","weather_hourly_nooutlier_nogaps = weather_hourly_nooutlier.fillna(method='ffill')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrcyLlaXQ_T5","colab_type":"code","colab":{}},"source":["temperature = weather_hourly_nooutlier_nogaps[\"TemperatureC\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A81lP4ZuRIg-","colab_type":"code","colab":{}},"source":["temperature.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZ1yDvYuRmke","colab_type":"text"},"source":["## Create Train and Test Datasets for Supervsed Learning\n","\n","With **supervised learning**, the model is given a set of data that will be used to **train** the model to predict a specific objectice. In this case, we will use a few simple time series features as well as outdoor air temperature to predict how much energy a building uses.\n","\n","For this demonstration, we will use three months of data from April, May, and June to prediction July."]},{"cell_type":"code","metadata":{"id":"W6CujBfuRPSp","colab_type":"code","colab":{}},"source":["training_months = [4,5,6]\n","test_months = [7]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snUjsHzdkW4a","colab_type":"text"},"source":["We can divide the data set by using the `datetime index` of the data frame and a function known as `.isin` to extract the months for the model"]},{"cell_type":"code","metadata":{"id":"KS8GOF_PR4W0","colab_type":"code","colab":{}},"source":["trainingdata = df_prediction_data[df_prediction_data.index.month.isin(training_months)]\n","testdata = df_prediction_data[df_prediction_data.index.month.isin(test_months)]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3PPiTEgSNpo","colab_type":"code","colab":{}},"source":["trainingdata.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EfJiggBzSVlX","colab_type":"code","colab":{}},"source":["testdata.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_hwubi-kkN-","colab_type":"text"},"source":["We can extract the training input data features that will go into the model and the training **label** data which is what are are targeting to predict. \n","\n","## Encoding Categorical Variables \n","\n","We use the pandas `.get_dummies()` function to change the temporal variables of *time of day* and *day of week* into categories that the model can use more effectively. This process is known as [enconding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)"]},{"cell_type":"code","metadata":{"id":"3sXkNOmESZFX","colab_type":"code","colab":{}},"source":["train_features = pd.concat([pd.get_dummies(trainingdata.index.hour), \n","                                     pd.get_dummies(trainingdata.index.dayofweek), \n","                                     pd.DataFrame(temperature[temperature.index.month.isin(training_months)].values)], axis=1).dropna()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pO6ULGsDk-3S","colab_type":"code","colab":{}},"source":["train_features.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zx8nN-a0qFB2","colab_type":"text"},"source":["## Train a K-Neighbor Regressor Model\n","\n","This model was chosen after following the process in the cheat sheet until a model that worked and provided good results was found."]},{"cell_type":"code","metadata":{"id":"fbd421YETWvW","colab_type":"code","colab":{}},"source":["model = KNeighborsRegressor().fit(np.array(train_features), np.array(trainingdata.values));\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGUKiipfTaz0","colab_type":"code","colab":{}},"source":["test_features = np.array(pd.concat([pd.get_dummies(testdata.index.hour),\n","                                    pd.get_dummies(testdata.index.dayofweek),\n","                                    pd.DataFrame(temperature[temperature.index.month.isin(test_months)].values)], axis=1).dropna())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELDwYHsqqVtw","colab_type":"text"},"source":["\n","\n","## Use the Model to predict for the *Test* period\n","\n","Then the model is given the `test_features` from the period which we want to predict. We can then merge those results and see how the model did"]},{"cell_type":"code","metadata":{"id":"S8OmtUk_YRqv","colab_type":"code","colab":{}},"source":["predictions = model.predict(test_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yy8PQBfbYUvk","colab_type":"code","colab":{}},"source":["predicted_vs_actual = pd.concat([testdata, pd.DataFrame(predictions, index=testdata.index)], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAOurMSjH3tk","colab_type":"code","colab":{}},"source":["predicted_vs_actual.columns = [\"Actual\", \"Predicted\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16esn6MzICDI","colab_type":"code","colab":{}},"source":["predicted_vs_actual.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rmX0oUrOYj2h","colab_type":"code","colab":{}},"source":["predicted_vs_actual.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbgcjsrHHyqb","colab_type":"code","colab":{}},"source":["trainingdata.columns = [\"Actual\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TWkEIbQY_0Q","colab_type":"code","colab":{}},"source":["predicted_vs_actual_plus_training = pd.concat([trainingdata, predicted_vs_actual], sort=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IT0zpaRv_VAP","colab_type":"code","colab":{}},"source":["predicted_vs_actual_plus_training.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXlyT5AZqoDC","colab_type":"text"},"source":["## Regression evaluation metrics\n","\n","In order to understand quanitatively how the model performed, we can use various evaluation metrics to understand how well the model compared to reality. \n","\n","In this situation, let's use the error metric [Mean Absolute Percentage Error (MAPE)](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) "]},{"cell_type":"code","metadata":{"id":"Slryz7Ze_WtE","colab_type":"code","colab":{}},"source":["# Calculate the absolute errors\n","errors = abs(predicted_vs_actual['Predicted'] - predicted_vs_actual['Actual'])\n","# Calculate mean absolute percentage error (MAPE) and add to list\n","MAPE = 100 * np.mean((errors / predicted_vs_actual['Actual']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikFCeMaG9LM6","colab_type":"code","colab":{}},"source":["MAPE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7L6TCfai9MdI","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}